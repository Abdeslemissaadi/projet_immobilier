from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator

import pandas as pd
import matplotlib.pyplot as plt
import os


def tune_random_forest(train, test, target_col, rf_metrics, rf_model, evaluator_rmse, evaluator_mae, evaluator_r2):
    # ====================================================================
    # BLOC 6 : OPTIMISATION ALL√âG√âE (Grid Search r√©duit)
    # ====================================================================

    print("\nüîß BLOC 6: OPTIMISATION AVANC√âE (Grid Search All√©g√©)")
    print("="*60)

    # SOLUTION 1 : Grid Search avec MOINS de combinaisons
    print("\n‚öôÔ∏è  Configuration du Grid Search ALL√âG√â pour Random Forest...")

    # Cr√©er le mod√®le de base
    rf_tuning = RandomForestRegressor(featuresCol="features", labelCol=target_col, seed=42)

    # Grille R√âDUITE (6 combinaisons au lieu de 18)
    paramGrid = (ParamGridBuilder()
        .addGrid(rf_tuning.numTrees, [50, 100])        # 2 valeurs au lieu de 3
        .addGrid(rf_tuning.maxDepth, [10, 15])         # 2 valeurs au lieu de 3
        .addGrid(rf_tuning.minInstancesPerNode, [1])   # 1 valeur au lieu de 2
        .build())

    print(f"üîç Test de {len(paramGrid)} combinaisons d'hyperparam√®tres (r√©duit pour √©viter crash)")

    # √âvaluateur
    evaluator_cv = RegressionEvaluator(labelCol=target_col, predictionCol="prediction", metricName="rmse")

    # Cross-Validator ALL√âG√â (2-fold au lieu de 3, parallelisme=2)
    cv = CrossValidator(
        estimator=rf_tuning,
        estimatorParamMaps=paramGrid,
        evaluator=evaluator_cv,
        numFolds=2,           # R√©duit de 3 √† 2 pour √©conomiser RAM
        parallelism=2,        # R√©duit de 4 √† 2
        seed=42
    )

    print(f"üìä Cross-validation 2-fold en cours (optimis√© m√©moire)...")
    print(f"‚è≥ Temps estim√©: ~1-2 minutes...")

    rf_train_rmse = rf_metrics["train_rmse"]
    rf_train_mae = rf_metrics["train_mae"]
    rf_train_r2 = rf_metrics["train_r2"]
    rf_test_rmse = rf_metrics["test_rmse"]
    rf_test_mae = rf_metrics["test_mae"]
    rf_test_r2 = rf_metrics["test_r2"]

    try:
        cv_model = cv.fit(train)
        
        # Meilleurs param√®tres
        best_rf_model = cv_model.bestModel
        print(f"\nüèÜ MEILLEURS HYPERPARAM√àTRES TROUV√âS:")
        print(f"   ‚Ä¢ numTrees: {best_rf_model.getNumTrees}")
        print(f"   ‚Ä¢ maxDepth: {best_rf_model.getMaxDepth()}")
        print(f"   ‚Ä¢ minInstancesPerNode: {best_rf_model.getMinInstancesPerNode()}")
        
        # √âvaluation du mod√®le optimis√©
        train_pred_opt = cv_model.transform(train)
        test_pred_opt = cv_model.transform(test)
        
        opt_train_rmse = evaluator_rmse.evaluate(train_pred_opt)
        opt_train_mae = evaluator_mae.evaluate(train_pred_opt)
        opt_train_r2 = evaluator_r2.evaluate(train_pred_opt)
        
        opt_test_rmse = evaluator_rmse.evaluate(test_pred_opt)
        opt_test_mae = evaluator_mae.evaluate(test_pred_opt)
        opt_test_r2 = evaluator_r2.evaluate(test_pred_opt)
        
        print(f"\nüìä R√âSULTATS MOD√àLE OPTIMIS√â (RF avec CV):")
        print(f"   TRAIN ‚Üí RMSE: {opt_train_rmse:.4f} | MAE: {opt_train_mae:.4f} | R¬≤: {opt_train_r2:.4f}")
        print(f"   TEST  ‚Üí RMSE: {opt_test_rmse:.4f} | MAE: {opt_test_mae:.4f} | R¬≤: {opt_test_r2:.4f}")
        
        # Comparaison avant/apr√®s optimisation
        improvement_rmse = ((rf_test_rmse - opt_test_rmse) / rf_test_rmse) * 100
        improvement_r2 = ((opt_test_r2 - rf_test_r2) / rf_test_r2) * 100
        
        print(f"\nüìà AM√âLIORATION APR√àS OPTIMISATION:")
        print(f"   ‚Ä¢ RMSE: {improvement_rmse:.2f}% de r√©duction")
        print(f"   ‚Ä¢ R¬≤: {improvement_r2:.2f}% d'am√©lioration")
        
        cv_success = True
        
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Grid Search √©chou√© (RAM insuffisante)")
        print(f"üí° Utilisation du mod√®le RF de base comme 'optimis√©'")
        
        # Utiliser le mod√®le RF d√©j√† entra√Æn√©
        opt_test_rmse = rf_test_rmse
        opt_test_mae = rf_test_mae
        opt_test_r2 = rf_test_r2
        opt_train_rmse = rf_train_rmse
        opt_train_mae = rf_train_mae
        opt_train_r2 = rf_train_r2
        
        test_pred_opt = rf_model.transform(test)
        improvement_rmse = 0
        improvement_r2 = 0
        
        cv_success = False

    # Tableau comparatif final (3 mod√®les)
    results_final = pd.DataFrame({
        'Mod√®le': ['R√©gression Lin√©aire', 'Random Forest (base)', 
                   'Random Forest (optimis√© CV)' if cv_success else 'Random Forest (base - utilis√©)'],
        'RMSE_Test': [rf_metrics["test_rmse"], rf_metrics["test_rmse"], opt_test_rmse],
        'MAE_Test': [rf_metrics["test_mae"], rf_metrics["test_mae"], opt_test_mae],
        'R¬≤_Test': [rf_metrics["test_r2"], rf_metrics["test_r2"], opt_test_r2],
        'Temps_Training': ['Rapide (~10s)', 'Moyen (~30s)', 
                           'Long (~1-2min)' if cv_success else 'Moyen (~30s)']
    })

    print(f"\nüìä TABLEAU COMPARATIF FINAL:")
    print(results_final.to_string(index=False))

    # Sauvegarder les r√©sultats
    results_final.to_csv("resultats_final_avec_cv.csv", index=False)
    print(f"\nüíæ R√©sultats sauvegard√©s: resultats_final_avec_cv.csv")

    # Sauvegarder le meilleur mod√®le
    os.makedirs("models", exist_ok=True)

    if cv_success:
        cv_model.write().overwrite().save("models/best_rf_model")
        print(f"üíæ Mod√®le optimis√© sauvegard√©: models/best_rf_model/")
    else:
        rf_model.write().overwrite().save("models/best_rf_model")
        print(f"üíæ Mod√®le RF de base sauvegard√©: models/best_rf_model/")

    # Visualisation finale comparative
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))

    # Graph 1: Comparaison RMSE des 3 mod√®les
    models_names = ['LR', 'RF Base', 'RF Optimis√©' if cv_success else 'RF Final']
    rmse_values = [rf_metrics["test_rmse"], rf_metrics["test_rmse"], opt_test_rmse]
    colors = ['skyblue', 'lightgreen', 'gold' if cv_success else 'lightgreen']
    axes[0].bar(models_names, rmse_values, color=colors, edgecolor='black', linewidth=1.5)
    axes[0].set_ylabel('RMSE (Test)', fontsize=12, fontweight='bold')
    axes[0].set_title('Comparaison RMSE - 3 Mod√®les', fontsize=14, fontweight='bold')
    axes[0].grid(axis='y', alpha=0.3)
    for i, v in enumerate(rmse_values):
        axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')

    # Graph 2: Comparaison R¬≤
    r2_values = [rf_metrics["test_r2"], rf_metrics["test_r2"], opt_test_r2]
    axes[1].bar(models_names, r2_values, color=colors, edgecolor='black', linewidth=1.5)
    axes[1].set_ylabel('R¬≤ Score (Test)', fontsize=12, fontweight='bold')
    axes[1].set_title('Comparaison R¬≤ - 3 Mod√®les', fontsize=14, fontweight='bold')
    axes[1].set_ylim([0, 1])
    axes[1].grid(axis='y', alpha=0.3)
    for i, v in enumerate(r2_values):
        axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')

    # Graph 3: Pr√©dictions vs R√©el (Meilleur mod√®le)
    sample_opt = test_pred_opt.sample(0.1, seed=42).toPandas()
    axes[2].scatter(sample_opt[target_col], sample_opt['prediction'], 
                    alpha=0.5, s=15, c='purple', edgecolors='black', linewidths=0.5)
    axes[2].plot([0, 5], [0, 5], 'r--', lw=2, label='Pr√©diction parfaite')
    axes[2].set_xlabel('Prix R√©el (√ó$100k)', fontsize=12)
    axes[2].set_ylabel('Prix Pr√©dit (√ó$100k)', fontsize=12)
    axes[2].set_title(f'{"RF Optimis√©" if cv_success else "RF Base"}: Pr√©dictions vs R√©el', 
                      fontsize=14, fontweight='bold')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('comparaison_3_modeles.png', dpi=300, bbox_inches='tight')
    plt.show()

    print("\n‚úÖ BLOC 6 OK: Optimisation termin√©e (version all√©g√©e)")

    return cv_success, opt_test_rmse, opt_test_r2
